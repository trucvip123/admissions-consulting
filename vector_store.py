import os
import logging
from typing import List, Dict
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from config import Config
from document_processor import DocumentProcessor
from query_expander import QueryExpander
from datetime import datetime
import hashlib
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VectorStore:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
        )
        self.vector_db = None
        self.query_expander = QueryExpander()

    def create_documents(self, documents: List[Dict]) -> List[Document]:
        """T·∫°o danh s√°ch Document t·ª´ d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v·ªõi metadata n√¢ng cao"""
        langchain_documents = []
        current_time = datetime.now()

        for doc in documents:
            # Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè
            chunks = self.text_splitter.split_text(doc['content'])

            # T·∫°o metadata n√¢ng cao
            file_info = self._extract_file_info(doc['filename'])

            for i, chunk in enumerate(chunks):
                # T·∫°o hash cho chunk ƒë·ªÉ tracking
                chunk_hash = hashlib.md5(chunk.encode()).hexdigest()[:8]

                # Metadata n√¢ng cao
                enhanced_metadata = {
                    'source': doc['filename'],
                    'chunk_id': i,
                    'total_chunks': len(chunks),
                    'chunk_hash': chunk_hash,
                    'load_time': current_time.isoformat(),
                    'file_size': len(doc['content']),
                    'chunk_size': len(chunk),
                    'file_type': file_info['type'],
                    'file_title': file_info['title'],
                    'file_year': file_info['year'],
                    'file_category': file_info['category'],
                    'processing_timestamp': current_time.timestamp()
                }

                langchain_documents.append(
                    Document(
                        page_content=chunk,
                        metadata=enhanced_metadata
                    )
                )

        logger.info(f"ƒê√£ t·∫°o {len(langchain_documents)} chunks t·ª´ {len(documents)} t√†i li·ªáu v·ªõi metadata n√¢ng cao")
        return langchain_documents

    def _extract_file_info(self, filename: str) -> Dict:
        """Tr√≠ch xu·∫•t th√¥ng tin t·ª´ t√™n file"""
        info = {
            'type': 'docx',
            'title': filename.replace('.docx', ''),
            'year': 'unknown',
            'category': 'general'
        }

        # Tr√≠ch xu·∫•t nƒÉm t·ª´ t√™n file
        year_match = re.search(r'20\d{2}', filename)
        if year_match:
            info['year'] = year_match.group()

        # Ph√¢n lo·∫°i t√†i li·ªáu d·ª±a tr√™n t√™n file
        filename_lower = filename.lower()
        if 'chi_tieu' in filename_lower or 'ch·ªâ ti√™u' in filename_lower:
            info['category'] = 'quota'
        elif 'quy_che' in filename_lower or 'quy ch·∫ø' in filename_lower:
            info['category'] = 'regulation'
        elif 'diem_chuan' in filename_lower or 'ƒëi·ªÉm chu·∫©n' in filename_lower:
            info['category'] = 'benchmark'
        elif 'thong_tin' in filename_lower or 'th√¥ng tin' in filename_lower:
            info['category'] = 'information'

        return info

    def build_vector_store(self, force_rebuild: bool = False):
        """X√¢y d·ª±ng c∆° s·ªü d·ªØ li·ªáu vector"""
        if not force_rebuild and os.path.exists(Config.VECTOR_DB_PATH):
            logger.info("ƒêang t·∫£i c∆° s·ªü d·ªØ li·ªáu vector hi·ªán c√≥...")
            self.vector_db = FAISS.load_local(
                Config.VECTOR_DB_PATH,
                self.embeddings,
                allow_dangerous_deserialization=True,
            )
            return

        logger.info("ƒêang x√¢y d·ª±ng c∆° s·ªü d·ªØ li·ªáu vector m·ªõi...")

        # X·ª≠ l√Ω t√†i li·ªáu
        processor = DocumentProcessor()
        documents = processor.process_all_documents()

        if not documents:
            logger.error("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ƒë·ªÉ x·ª≠ l√Ω!")
            return

        # T·∫°o documents cho langchain
        langchain_documents = self.create_documents(documents)

        # T·∫°o vector store
        self.vector_db = FAISS.from_documents(langchain_documents, self.embeddings)

        # L∆∞u vector store
        os.makedirs(Config.VECTOR_DB_PATH, exist_ok=True)
        self.vector_db.save_local(Config.VECTOR_DB_PATH)

        logger.info(f"ƒê√£ l∆∞u c∆° s·ªü d·ªØ li·ªáu vector t·∫°i: {Config.VECTOR_DB_PATH}")

    def search(self, query: str, k: int = 5, use_query_expansion: bool = True) -> List[Dict]:
        """T√¨m ki·∫øm th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi v·ªõi t√πy ch·ªçn m·ªü r·ªông truy v·∫•n"""
        if not self.vector_db:
            logger.error("C∆° s·ªü d·ªØ li·ªáu vector ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o!")
            return []

        try:
            logger.info(f"üîç T√¨m ki·∫øm: '{query}' (k={k}, expansion={use_query_expansion})")
            
            # M·ªü r·ªông truy v·∫•n n·∫øu ƒë∆∞·ª£c b·∫≠t
            if use_query_expansion:
                expanded_queries = self.query_expander.expand_query(query, method="combined")
                logger.info(f"üìà S·ª≠ d·ª•ng {len(expanded_queries)} truy v·∫•n m·ªü r·ªông")
                
                # T√¨m ki·∫øm v·ªõi t·∫•t c·∫£ c√°c truy v·∫•n m·ªü r·ªông
                all_results = []
                for exp_query in expanded_queries:
                    try:
                        exp_results = self.vector_db.similarity_search_with_score(exp_query, k=k//2)
                        all_results.extend(exp_results)
                    except Exception as e:
                        logger.warning(f"L·ªói khi t√¨m ki·∫øm v·ªõi query '{exp_query}': {e}")
                
                # S·∫Øp x·∫øp theo score v√† l·∫•y top k
                all_results.sort(key=lambda x: x[1])  # S·∫Øp x·∫øp theo score (th·∫•p h∆°n = t·ªët h∆°n)
                results = all_results[:k]
            else:
                results = self.vector_db.similarity_search_with_score(query, k=k)

            formatted_results = []
            for doc, score in results:
                formatted_results.append({
                    'content': doc.page_content,
                    'source': doc.metadata.get('source', 'Unknown'),
                    'score': float(score),
                    'chunk_id': doc.metadata.get('chunk_id', 0),
                    'total_chunks': doc.metadata.get('total_chunks', 0),
                    # Metadata n√¢ng cao
                    'chunk_hash': doc.metadata.get('chunk_hash', ''),
                    'load_time': doc.metadata.get('load_time', ''),
                    'file_size': doc.metadata.get('file_size', 0),
                    'chunk_size': doc.metadata.get('chunk_size', 0),
                    'file_type': doc.metadata.get('file_type', 'unknown'),
                    'file_title': doc.metadata.get('file_title', ''),
                    'file_year': doc.metadata.get('file_year', 'unknown'),
                    'file_category': doc.metadata.get('file_category', 'general'),
                    'processing_timestamp': doc.metadata.get('processing_timestamp', 0)
                })

            logger.info(f"‚úÖ T√¨m th·∫•y {len(formatted_results)} k·∫øt qu·∫£ t·ª´ {len(set(r['source'] for r in formatted_results))} t√†i li·ªáu")
            for i, result in enumerate(formatted_results[:3], 1):
                logger.info(f"   {i}. {result['file_title']} ({result['file_category']}, {result['file_year']}) - score: {result['score']:.4f}")
                logger.info(f"      Chunk {result['chunk_id']}/{result['total_chunks']} - Hash: {result['chunk_hash']}")

            return formatted_results
        except Exception as e:
            logger.error(f"L·ªói khi t√¨m ki·∫øm: {str(e)}")
            return []

    def get_statistics(self) -> Dict:
        """L·∫•y th·ªëng k√™ v·ªÅ c∆° s·ªü d·ªØ li·ªáu vector"""
        if not self.vector_db:
            return {'status': 'not_initialized'}
        
        try:
            # L·∫•y th√¥ng tin chi ti·∫øt t·ª´ vector store
            total_vectors = len(self.vector_db.index_to_docstore_id)
            
            # Th·ªëng k√™ theo category
            categories = {}
            years = {}
            file_sizes = []
            
            # L·∫•y metadata t·ª´ t·∫•t c·∫£ documents (n·∫øu c√≥ th·ªÉ)
            try:
                # Th·ª≠ l·∫•y m·ªôt s·ªë documents ƒë·ªÉ ph√¢n t√≠ch
                sample_docs = self.vector_db.similarity_search("", k=min(100, total_vectors))
                for doc in sample_docs:
                    category = doc.metadata.get('file_category', 'unknown')
                    year = doc.metadata.get('file_year', 'unknown')
                    file_size = doc.metadata.get('file_size', 0)
                    
                    categories[category] = categories.get(category, 0) + 1
                    years[year] = years.get(year, 0) + 1
                    if file_size > 0:
                        file_sizes.append(file_size)
            except:
                pass
            
            return {
                'status': 'initialized',
                'total_vectors': total_vectors,
                'categories': categories,
                'years': years,
                'avg_file_size': sum(file_sizes) // len(file_sizes) if file_sizes else 0,
                'total_files': len(set(years.keys()) - {'unknown'}) if years else 0
            }
        except Exception as e:
            logger.error(f"L·ªói khi l·∫•y th·ªëng k√™: {str(e)}")
            return {'status': 'error', 'error': str(e)}


if __name__ == "__main__":
    vector_store = VectorStore()
    vector_store.build_vector_store(force_rebuild=True)

    # Test t√¨m ki·∫øm
    test_query = "ch·ªâ ti√™u tuy·ªÉn sinh 2025"
    results = vector_store.search(test_query)

    print(f"\nK·∫øt qu·∫£ t√¨m ki·∫øm cho: '{test_query}'")
    for i, result in enumerate(results, 1):
        print(f"\n--- K·∫øt qu·∫£ {i} ---")
        print(f"Ngu·ªìn: {result['source']}")
        print(f"ƒêi·ªÉm: {result['score']:.4f}")
        print(f"N·ªôi dung: {result['content'][:200]}...")
